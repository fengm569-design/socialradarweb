name: scrape-zhihu

on:
  workflow_dispatch:
  schedule:
    - cron: "1 * * * *"


# 防止并发运行导致 push 互相冲突
concurrency:
  group: scrape-zhihu
  cancel-in-progress: true

# 允许 workflow 把生成的数据 push 回仓库
permissions:
  contents: write

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          # 关键：拉全历史，后面 rebase 才不会出问题
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install playwright pandas
          python -m playwright install --with-deps chromium

      # （可选）如果你需要登录态：把 storage_state.json 的 base64 放到 GitHub Secrets
      # Secret 名字：ZHIHU_STORAGE_STATE_B64
      - name: Restore storage_state.json (optional)
        env:
          ZHIHU_STORAGE_STATE_B64: ${{ secrets.ZHIHU_STORAGE_STATE_B64 }}
        run: |
          if [ -n "$ZHIHU_STORAGE_STATE_B64" ]; then
            echo "$ZHIHU_STORAGE_STATE_B64" | base64 -d > storage_state.json
            echo "storage_state.json restored"
          else
            echo "No storage state provided"
          fi

      - name: Run scraper
        env:
          KEYWORDS: '["自动化学会"]'
          MAX_RESULTS_PER_KEYWORD: "50"
          HEADLESS: "true"
        run: |
          mkdir -p data
          python scripts/zhihu_scrape_once.py

      - name: Commit data
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # 把生成的数据加入版本控制
          git add data/*.json data/*.csv || true

          # 没有变化就不提交
          git commit -m "update zhihu data" || echo "No changes"

          # 关键：先同步远端再推送，避免 main -> main (fetch first)
          git pull --rebase origin main

          # 推送到 main
          git push origin main
